## Context

The signal store has five ingestion layers (syndication, research, narrative, events, personal-graph) that collect raw signals. AI research adds a sixth layer that uses AI-powered search and synthesis APIs to produce higher-signal, pre-interpreted intelligence. Two APIs serve different purposes: Perplexity for synthesized answers, Tavily for targeted discovery.

## Goals / Non-Goals

**Goals:**
- Define query generation logic that derives research questions from user profiles
- Define types for AI research queries, responses, and citations
- Add "ai-research" as a signal layer
- Support both batch (daily run) and on-demand (deep-dive) research modes
- Deduplicate queries across users with overlapping profiles

**Non-Goals:**
- Implementing the actual API client code (types and interfaces only)
- Query optimization/caching strategies (future)
- Custom fine-tuning of research prompts (start with templates, iterate)

## Decisions

### Decision 1: Two research providers for two purposes

Perplexity Sonar handles "big picture" questions ("What should a wearables PM know today?") — it searches, reads, and synthesizes. Tavily handles "needle in haystack" questions ("Did [company] announce anything this week?") — it finds specific results with high precision. Using both gives breadth and depth. Alternative: Perplexity only — rejected because Perplexity is overkill for simple targeted lookups and costs more per query.

### Decision 2: Query generation from profile templates

Queries are generated by filling templates with user profile data. Templates map to intelligence goals, impress list interests, peer org monitoring, etc. Example template: "What developments in {industry} in the last 24 hours should a {role} at a {company_size} {company_type} company know about?" This is simple, predictable, and tunable. Alternative: LLM-generated queries — possible future enhancement but adds latency and cost to the query generation step itself.

### Decision 3: Deep-dive reuses the same pipeline

When a user requests a deep-dive on a briefing item, the system generates a Perplexity query based on the item + user profile context and returns the synthesized answer. This reuses the same AI research infrastructure for both batch and on-demand modes. The deep-dive result is also stored in the signal store for future reference.

### Decision 4: Query deduplication by content hash

Before firing a query, hash it and check if the same query was already run in the current daily batch. If two users would generate "What happened at Acme Corp this week?", run it once and tag the resulting signal with provenance for both users.

## Risks / Trade-offs

- **API cost scaling** — 5-10 Perplexity queries per user per day adds up. Mitigation: aggressive query dedup across users; cluster similar users and share queries.
- **Query quality** — Template-based queries may miss nuance. Mitigation: start with templates, add LLM-generated queries as an enhancement layer once we see what templates miss.
- **Perplexity rate limits** — Sonar API has rate limits on lower tiers. Mitigation: batch queries, respect rate limits, fall back to Tavily for overflow.
